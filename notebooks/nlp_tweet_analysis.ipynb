{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jparep/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jparep/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jparep/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ncessary libraries\n",
    "import nltk.downloader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "FAKE_CSV_PATH = '/home/jparep/proj/nlp-tweet-analysis/data/raw/fake.csv'\n",
    "REAL_CSV_PATH = '/home/jparep/proj/nlp-tweet-analysis/data/raw/true.csv'\n",
    "MODEL_PATH = '/home/jparep/proj/nlp-tweet-analysis/model/model.pkl'\n",
    "VECTORIZER_PATH = '/home/jparep/proj/nlp-tweet-analysis/model/vectorizer.pkl'\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Initliaze stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files(real_csv, fake_csv):\n",
    "    \"\"\"Load data and label fake and real and return concatenate dataframe\"\"\"\n",
    "    df_fake = pd.read_csv(fake_csv)\n",
    "    df_real = pd.read_csv(real_csv)\n",
    "    \n",
    "    df_fake['label'] = 'fake'\n",
    "    df_real['label'] = 'real'\n",
    "    \n",
    "    df_concat = pd.concat([df_fake, df_real], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess data\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9]+', ' ', text).lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load data and preprocess. Make fake and real to 1 and 0 respectively\"\"\"\n",
    "    df = read_csv_files(REAL_CSV_PATH, FAKE_CSV_PATH)\n",
    "    df = df[['text', 'label']]\n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "    df[\"label\"] = df[\"label\"].map({\"real\": 0, \"fake\": 1})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anyone who uses a driving service such as Uber...</td>\n",
       "      <td>1</td>\n",
       "      <td>anyone us driving service uber lyft take risk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WASHINGTON (Reuters) - President Donald Trump ...</td>\n",
       "      <td>0</td>\n",
       "      <td>washington reuters president donald trump said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISTANBUL (Reuters) - A Turkish court on Wednes...</td>\n",
       "      <td>0</td>\n",
       "      <td>istanbul reuters turkish court wednesday order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dinesh D Souza s  Hillary s America  will debu...</td>\n",
       "      <td>1</td>\n",
       "      <td>dinesh souza hillary america debut theater jul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When on the campaign trail, those running for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>campaign trail running office like use music b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Anyone who uses a driving service such as Uber...      1   \n",
       "1  WASHINGTON (Reuters) - President Donald Trump ...      0   \n",
       "2  ISTANBUL (Reuters) - A Turkish court on Wednes...      0   \n",
       "3  Dinesh D Souza s  Hillary s America  will debu...      1   \n",
       "4  When on the campaign trail, those running for ...      1   \n",
       "\n",
       "                                      processed_text  \n",
       "0  anyone us driving service uber lyft take risk ...  \n",
       "1  washington reuters president donald trump said...  \n",
       "2  istanbul reuters turkish court wednesday order...  \n",
       "3  dinesh souza hillary america debut theater jul...  \n",
       "4  campaign trail running office like use music b...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse preprocessed data\n",
    "df = load_and_preprocess_data()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validate-test split\n",
    "def train_valid_test_split(X, y, train_size=0.7, valid_size=0.15, test_size=0.15):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(valid_size + test_size), random_state=RANDOM_SEED)\n",
    "    \n",
    "    ratio = valid_size / (valid_size + test_size)\n",
    "    \n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=(1.0 - ratio), random_state=RANDOM_SEED)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call method\n",
    "X = df['processed_text']\n",
    "y = df['label']\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = train_valid_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xv_train, xv_valid, xv_test, vectorizer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# call method\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m xv_train, xv_valid, xv_test \u001b[38;5;241m=\u001b[39m vectorize_data(X_train, X_valid, X_test)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Vectorize data\n",
    "def vectorize_data(X_train, X_valid, X_test):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    xv_train = vectorizer.fit_transform(X_train)\n",
    "    xv_valid = vectorizer.fit_transform(X_valid)\n",
    "    xv_test = vectorizer.fit_transform(X_test)\n",
    "    \n",
    "    with open(VECTORIZER_PATH, 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    \n",
    "    return xv_train, xv_valid, xv_test, vectorizer\n",
    "\n",
    "# call method\n",
    "xv_train, xv_valid, xv_test, vectorizer = vectorize_data(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
